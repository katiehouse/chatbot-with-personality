{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2DoubleHeadsModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (multiple_choice_head): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (activation): Identity()\n",
       "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (last_dropout): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50284, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "genre_dict = {'comedy': '<comedy>',\n",
    " 'sport': '<sport>',\n",
    " 'biography': '<biography>',\n",
    " 'romance': '<romance>',\n",
    " 'action': '<action>',\n",
    " 'adventure': '<adventure>',\n",
    " 'drama': '<drama>',\n",
    " 'sci-fi': '<sci-fi>',\n",
    " 'family': '<family>',\n",
    " 'fantasy': '<fantasy>',\n",
    " 'musical': '<musical>',\n",
    " 'crime': '<crime>',\n",
    " 'thriller': '<thriller>',\n",
    " 'short': '<short>',\n",
    " 'western': '<western>',\n",
    " 'documentary': '<documentary>',\n",
    " 'horror': '<horror>',\n",
    " 'animation': '<animation>',\n",
    " 'film-noir': '<film-noir>',\n",
    " 'music': '<music>',\n",
    " 'war': '<war>',\n",
    " 'mystery': '<mystery>'}\n",
    "\n",
    "genres = genre_dict.keys()\n",
    "\n",
    "special_tokens = [\"<speaker1>\", \"<speaker2>\"] + [\"<\" + genre + \">\" for genre in genres]\n",
    "\n",
    "SPECIAL_TOKENS = {\"bos_token\": \"<bos>\", \"eos_token\": \"<eos>\", \"additional_special_tokens\": special_tokens, \"pad_token\": \"<pad>\"}\n",
    "\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain only final fc layer of first head for language modeling task\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.lm_head = nn.Linear(model.lm_head.in_features, len(tokenizer))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import transformers\n",
    "from itertools import chain\n",
    "\n",
    "#handles one personality and history sentence\n",
    "def build_inputs_simple(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "\n",
    "    bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "    sequence = [[bos] + persona] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                        for i, s in enumerate(sequence[1:])]\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                        for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "def pad(x, padding, padding_length):\n",
    "    return x + [padding] * (padding_length - len(x))\n",
    "\n",
    "def randomIncorrectReply(df, row):\n",
    "    return df[df['movie_name'] != row['movie_name']]['sentence_2'].sample(1).iloc[0]\n",
    "\n",
    "def processRow(df, idx, genre_dict, max_len):\n",
    "\n",
    "    row = df.iloc[idx]\n",
    "\n",
    "    # Let's define our contexts and special tokens\n",
    "    genre = eval(row['genre'])\n",
    "    if genre is None:\n",
    "        genre = random.choice(list(genre_dict.keys()))\n",
    "    if isinstance(genre, list):\n",
    "        genre = genre[0]\n",
    "    persona = [genre_dict[genre]]\n",
    "    #history = process_history(row).split()\n",
    "    history = [row['sentence_1'].split()]\n",
    "    correct_reply = row['sentence_2'].split()\n",
    "    incorrect_reply = randomIncorrectReply(df, row).split()\n",
    "\n",
    "    words_1, segments_1, position_1, sequence_1 = build_inputs_simple(persona, history, correct_reply)\n",
    "    words_2, segments_2, position_2, sequence_2 = build_inputs_simple(persona, history, incorrect_reply)\n",
    "\n",
    "    # Tokenize words and segments embeddings:\n",
    "    words_1 = tokenizer.convert_tokens_to_ids(words_1) \n",
    "    words_2 = tokenizer.convert_tokens_to_ids(words_2)\n",
    "    segments_1 = tokenizer.convert_tokens_to_ids(segments_1)\n",
    "    segments_2 = tokenizer.convert_tokens_to_ids(segments_2)\n",
    "\n",
    "    lm_targets_1 = ([-1] * sum(len(s) for s in sequence_1[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence_1[-1][1:])\n",
    "    lm_targets_2 = [-1] * len(words_2)\n",
    "\n",
    "    # Store the position of the last tokens for the next-sentence prediction loss\n",
    "    last_token_1 = len(words_1) - 1\n",
    "    last_token_2 = len(words_2) - 1\n",
    "\n",
    "    (words_1, words_2,\n",
    "    segments_1, segments_2) = [pad(x, tokenizer.convert_tokens_to_ids('<pad>'), max_len)\n",
    "                                   for x in (words_1, words_2,\n",
    "                                             segments_1, segments_2)]\n",
    "\n",
    "    (lm_targets_1, lm_targets_2) = [pad(x, -1, max_len) for x in (lm_targets_1, lm_targets_2)]\n",
    "\n",
    "    # And gather reply and distractor inputs to build the input tensors:\n",
    "    # words tokens\n",
    "    input_ids = torch.tensor([words_1, words_2])#.unsqueeze(0)\n",
    "    # segment tokens\n",
    "    token_type_ids = torch.tensor([segments_1, segments_2])#.unsqueeze(0)\n",
    "    # Last tokens location\n",
    "    mc_token_ids = torch.tensor([last_token_1, last_token_2], dtype=torch.long)#.unsqueeze(0)\n",
    "    # Language modeling labels\n",
    "    lm_labels = torch.tensor([lm_targets_1, lm_targets_2], dtype=torch.long)#.unsqueeze(0)\n",
    "    # Next-sentence prediction labels\n",
    "    mc_labels = torch.zeros(1, dtype=torch.long) # Gold reply is 1st (index 0)\n",
    "\n",
    "    return input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels\n",
    "    \n",
    "class DialogueDataset(Dataset):\n",
    "    \"\"\"Movie dialogue conversation dataset.\"\"\"\n",
    "    #data processing functions\n",
    "\n",
    "    def __init__(self, csv_file, max_sent_len, min_sent_len, process_fun):\n",
    "        #loading data and some processing\n",
    "        df = pd.read_csv(csv_file, low_memory = False)\n",
    "        genres = [genre for genre in df.columns[22:43]]\n",
    "\n",
    "        #remove short or missing sentences\n",
    "        df = df[pd.notnull(df['sentence_1'])]\n",
    "        df = df[pd.notnull(df['sentence_2'])]\n",
    "        df = df[df['sentence_1'].apply(lambda x: len(x.split()) >= min_sent_len)]\n",
    "        df = df[df['sentence_2'].apply(lambda x: len(x.split()) >= min_sent_len)]\n",
    "        \n",
    "        #trim sentences to max length\n",
    "        df['sentence_1'] = df['sentence_1'].apply(lambda x: x.split()[:max_sent_len]).apply(lambda x: \" \".join(x))\n",
    "        df['sentence_2'] = df['sentence_2'].apply(lambda x: x.split()[:max_sent_len]).apply(lambda x: \" \".join(x))\n",
    "\n",
    "        self.genre_dict = dict.fromkeys(genres, \"\")\n",
    "        for key in genre_dict:\n",
    "            self.genre_dict[key] = '<' + key + \">\" \n",
    "        self.X = df\n",
    "        self.genre_dict['mystery'] = '<mystery>'\n",
    "        self.processRow = process_fun\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        return self.processRow(self.X, idx, self.genre_dict, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101807"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = '/Users/mm06832/Documents/Classes/CS585/final_project/chatbot-with-personality/data/processed_data_final.csv'\n",
    "\n",
    "dataset = DialogueDataset(csv_file = csv_file, max_sent_len = 20, min_sent_len = 5, process_fun = processRow)\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:  1596\n"
     ]
    }
   ],
   "source": [
    "#may need to adjust this to avoid target leakage\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "batch_size = 32\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = random.randint(1, 10000)\n",
    "print(\"random seed: \", random_seed)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import WarmupLinearSchedule as get_linear_schedule_with_warmup\n",
    "\n",
    "# Parameters: (some from hugging face repo)\n",
    "lr = 1e-3\n",
    "max_grad_norm = 1.0\n",
    "num_training_steps = 1000\n",
    "num_warmup_steps = 100\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "\n",
    "lm_losses = []\n",
    "mc_losses = []\n",
    "total_losses = []\n",
    "\n",
    "lm_losses_val = []\n",
    "mc_losses_val = []\n",
    "total_losses_val = []\n",
    "\n",
    "iters = 0\n",
    "lm_coef = 2.0\n",
    "mc_coef = 1.0\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/2546]\tLoss LM: 15.2908\tLoss MC: 0.9488\tLoss total:31.5305\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        model.train()\n",
    "        \n",
    "        batch_size = len(data)\n",
    "        \n",
    "        input_ids = data[0]\n",
    "        token_type_ids = data[1]\n",
    "        mc_token_ids = data[2]\n",
    "        lm_labels = data[3]\n",
    "        mc_labels = data[4]\n",
    "\n",
    "        output = model(input_ids, mc_token_ids=mc_token_ids, mc_labels = mc_labels, token_type_ids = token_type_ids, lm_labels = lm_labels)\n",
    "\n",
    "        lm_loss = output[0]\n",
    "        mc_loss = output[1]\n",
    "\n",
    "        total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss LM: %.4f\\tLoss MC: %.4f\\tLoss total:%.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     lm_loss.item(), mc_loss.item(), total_loss.item()))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        lm_losses.append(lm_loss.item())\n",
    "        mc_losses.append(mc_loss.item())\n",
    "        total_losses.append(total_loss.item())\n",
    "        \n",
    "        iters +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Testing..\")\n",
    "model.eval()\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    output = model(data)\n",
    "\n",
    "    lm_loss = output[0]\n",
    "    mc_loss = output[1] \n",
    "    lm_scores = output[2]\n",
    "    mc_scores = output[3]\n",
    "\n",
    "    total_loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "\n",
    "    # Output validation stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss LM: %.4f\\tLoss MC: %.4f\\tLoss total:%.4f'\n",
    "                  % (epoch, num_epochs, i, len(val_loader),\n",
    "                     lm_loss.item(), mc_loss.item(), total_loss.item()))\n",
    "\n",
    "    # Save Losses for plotting later\n",
    "    lm_losses_val.append(lm_loss.item())\n",
    "    mc_losses_val.append(mc_loss.item())\n",
    "    total_losses_val.append(total_loss.item())\n",
    "\n",
    "    iters +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#haven't touched yet \n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "# Here is how to use this function for top-p sampling\n",
    "temperature = 1.0\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "\n",
    "# Get logits with a forward pass in our model (input is pre-defined)\n",
    "logits = model(input)\n",
    "\n",
    "# Keep only the last token predictions of the first batch item (batch size 1), apply a temperature coefficient and filter\n",
    "logits = logits[0, -1, :] / temperature\n",
    "filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "\n",
    "# Sample from the filtered distribution\n",
    "probabilities = F.softmax(filtered_logits, dim=-1)\n",
    "next_token = torch.multinomial(probabilities, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation / Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
